{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfSb7bAmpnCM1AMPspsV22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan556gaur/map-reduce-filter/blob/main/BatchNormalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "print(tf.config.list_physical_devices('CPU'))"
      ],
      "metadata": {
        "id": "8ZA01CiMtECj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14397463-aa9e-4b19-c44e-d5b222dfcf9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train,y_train), (x_test,y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "SldapdmJBlBL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTmodel(tf.keras.Model):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(MNISTmodel, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense = tf.keras.layers.Dense(512, activation=\"relu\", name=\"d1\")\n",
        "    self.drop = tf.keras.layers.Dropout(0.2)\n",
        "    self.prediction = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"d2\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.flatten(inputs)\n",
        "    x = self.dense(x)\n",
        "    x = self.drop(x)\n",
        "    return self.prediction(x)\n",
        "\n",
        "mnist_model = MNISTmodel()\n",
        "\n",
        "batch_size = 32\n",
        "steps_per_epoch = len(x_train)//batch_size\n",
        "print(steps_per_epoch)\n",
        "mnist_model.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "mnist_model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
        "\n",
        "mnist_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "u2a0hcRNPRBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d452fbb2-da7f-4a5a-a3de-534fb9945e32"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.9038 - accuracy: 0.8795\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4345 - accuracy: 0.9060\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4071 - accuracy: 0.9111\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3885 - accuracy: 0.9145\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3715 - accuracy: 0.9182\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3491 - accuracy: 0.9216\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3345 - accuracy: 0.9265\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3263 - accuracy: 0.9271\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3247 - accuracy: 0.9294\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3289 - accuracy: 0.9308\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3098 - accuracy: 0.9469\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3098161518573761, 0.9469000101089478]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "class MNISTmodel(tf.keras.Model):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(MNISTmodel, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense = tf.keras.layers.Dense(512, activation=\"relu\", name=\"d1\")\n",
        "    self.drop = tf.keras.layers.Dropout(0.2)\n",
        "    BatchNormalization()\n",
        "    self.prediction = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"d2\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.flatten(inputs)\n",
        "    x = self.dense(x)\n",
        "    x = self.drop(x)\n",
        "    return self.prediction(x)\n",
        "\n",
        "mnist_model = MNISTmodel()\n",
        "\n",
        "batch_size=[32,64,128,256]\n",
        "\n",
        "for batch in batch_size:\n",
        "  steps_per_epoch = len(x_train)//batch\n",
        "  print(steps_per_epoch)\n",
        "  mnist_model.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "  mnist_model.fit(x_train, y_train, batch_size=batch, epochs=5)\n",
        "\n",
        "  mnist_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk4QxFZvYRrX",
        "outputId": "32c75a85-9283-438a-8216-dff65bd5e735"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.7283 - accuracy: 0.8740\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4526 - accuracy: 0.9069\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4099 - accuracy: 0.9103\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3864 - accuracy: 0.9128\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3695 - accuracy: 0.9199\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.9413\n",
            "937\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 4s 3ms/step - loss: 0.3236 - accuracy: 0.9350\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.2740 - accuracy: 0.9394\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2625 - accuracy: 0.9422\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2487 - accuracy: 0.9422\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.2321 - accuracy: 0.9467\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2864 - accuracy: 0.9541\n",
            "468\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.2090 - accuracy: 0.9550\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1815 - accuracy: 0.9567\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.1775 - accuracy: 0.9575\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1695 - accuracy: 0.9600\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1665 - accuracy: 0.9607\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2962 - accuracy: 0.9643\n",
            "234\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - 2s 3ms/step - loss: 0.1465 - accuracy: 0.9663\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1359 - accuracy: 0.9676\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1312 - accuracy: 0.9685\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9693\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 1s 3ms/step - loss: 0.1263 - accuracy: 0.9693\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3043 - accuracy: 0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batch normalization helps stabilize and accelerate the training of neural networks. It normalizes the activations of each layer, reducing the internal covariate shift and allowing for faster convergence. This can lead to faster training, better generalization, and potentially improved performance on validation and test datasets.**\n",
        "### **The model with batch normalization may show better validation accuracy and faster convergence compared to the model without batch normalization**"
      ],
      "metadata": {
        "id": "e--nFIQ6UWvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effect of Batch Size on Training Dynamics and Performance:\n",
        "When using batch normalization, the choice of batch size can influence the training process:\n",
        "\n",
        "Smaller Batch Sizes:\n",
        "\n",
        "With smaller batch sizes, the noise in the gradient estimates is higher, which can lead to more erratic updates to model parameters.\n",
        "However, batch normalization can help mitigate this noise by normalizing the activations within each batch.\n",
        "Training with smaller batch sizes might require more iterations to converge, but the network can adapt faster to new examples within each batch.\n",
        "Larger Batch Sizes:\n",
        "\n",
        "Larger batch sizes can provide more stable gradient estimates and might lead to smoother convergence.\n",
        "However, extremely large batch sizes might cause convergence to a suboptimal solution due to overfitting on the specific batch.\n",
        "Batch normalization can still help stabilize training by ensuring that activations remain centered and scaled.\n",
        "Advantages of Batch Normalization:\n",
        "\n",
        "Faster Convergence: Batch normalization helps in faster convergence of training by reducing internal covariate shift, allowing higher learning rates, and enabling efficient use of optimization algorithms.\n",
        "\n",
        "Stabilized Gradients: Batch normalization helps stabilize gradient values during training, preventing the vanishing gradient problem. This is especially important in deep networks.\n",
        "\n",
        "Regularization Effect: Batch normalization introduces a slight regularization effect, which can help prevent overfitting to the training data.\n",
        "\n",
        "Reduced Dependency on Initialization: Batch normalization reduces the sensitivity of the model to the initial parameter values, making it less dependent on careful weight initialization.\n",
        "\n",
        "Potential Limitations of Batch Normalization:\n",
        "\n",
        "Batch Size Dependency: While batch normalization provides some degree of normalization within each batch, extremely small batch sizes can still result in unstable training dynamics.\n",
        "\n",
        "Inference Complexity: During inference, batch normalization requires computing batch statistics (mean and variance), which can add computational overhead. Techniques like \"running averages\" can be used to mitigate this.\n",
        "\n",
        "Normalization Conflicts: In some cases, batch normalization may not work well if used in combination with other normalization techniques or if the dataset is already well-normalized.\n",
        "\n",
        "Mode Collapse in Generative Models: In certain generative models, batch normalization can lead to mode collapse, where the generator focuses on a subset of data modes.\n",
        "\n",
        "Hyperparameter Tuning: Batch normalization introduces additional hyperparameters (gamma and beta) that require tuning, though their default values often work well in practice."
      ],
      "metadata": {
        "id": "AAFTl7C4WrCX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zc8izTbWvXj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}